{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import pickle\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from fractions import Fraction\n",
    "import os.path\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{1. Introduction and Background}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are often reasons why we can't eat what we want: sometimes we don't have it in our fridge; sometimes those eating to us are allergic to an ingredient in our desired food; sometimes we are on a plane, and plane food doesn't cut it.  So, we decided to ask the NP hard question \"What DOES cut it?  How can I eat what I want given the circumstances I am in!?\"\n",
    "\n",
    "We might be in a position to choose amongst many ingredients to make any recipe we like, but these times are few and far between with young poor college students with potentially not much more than beans, rice, and raman.  So is there any hope for us?  Maybe.\n",
    "\n",
    "If we want to overcome the nutrition crisis, we need solutions.  Fast.  We need a way to decide what we can eat, given the constraints we are in.  Don't have apples?  Constraint.  Are you a diabetic?  Constraint.  Will you settle for nothing less than a mexican meal?  Constraint.\n",
    "\n",
    "In order to break out of the constraints of what we CANT have, we need the liberating cabability to see what we CAN have.  This will ulitimately be solved through machine learning, data science, and magic.  But in order for those things to happen, we need a ground to build upon: we need data.\n",
    "\n",
    "Due to the nature of the question \"What can I eat that will fit my nutritional needs, diatary constraints, and fit my current cravings?\", we need to have recipes, and corresponding nutritional data.  How does one come by these you may ask?  Scraping!  We delve into the world of numbers and human error in the following story.\n",
    "\n",
    "Let me take you on a mouth watering and brain hurting adventure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{2.A Data Collection: Recipes and Ingredients Scraping}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are we here?  Ah!  To collect recipes.\n",
    "\n",
    "When one first googles 'recipes'... one stops.  Let it suffice to say that the formatting, information, and quality of online recipe databases differs greatly.  There is little order or consistancy, even across a single website.  So, rather than suffer greatly trying to use Selinum and Beautiful soup, we decided to search out recipes in text format only, and only suffer a little- opting for our pain to come from Regex and a little bit of Beautiful Soup.\n",
    "\n",
    "What follows is some code we used to scrape the text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recipes():\n",
    "    ''' Gets all the recipes and saves them in a pickle.  Doesn't scrape\n",
    "    the website if the information is already there.'''\n",
    "\n",
    "    # if the website has been scraped, doesn't scrape it again.\n",
    "    # return the scraped contents\n",
    "    if os.path.exists('recipes.pickle'): # checks if the folder already exists\n",
    "        print(\"folder already here: returning contents\")\n",
    "        with open('recipes.pickle','rb') as f:\n",
    "            recipes = pickle.load(f) # load the saved contents \n",
    "            return recipes\n",
    "    # otherwise, scrapes the website, pickles the information, and \n",
    "    # returns the contents\n",
    "    else:\n",
    "        print(\"folder not here yet: creating contents\")\n",
    "        text_data = [] # create the list to store the contents\n",
    "        for i in type_list:\n",
    "            time.sleep(.25)\n",
    "            # gets the text files from the links on the website\n",
    "            contents = requests.get(f\"http://mc6help.tripod.com/RecipeLibrary/{i}\") \n",
    "            text_data.append(contents.text) # appends the contents to the list\n",
    "            \n",
    "        with open('recipes.pickle','wb') as f:\n",
    "            pickle.dump(text_data,f) # save the contents\n",
    "            \n",
    "        return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_HTML_extensions(filename = 'recipes.html'):\n",
    "    \"\"\"Return a list of the names of the text file extensions from the recipe website.\"\"\"\n",
    "    extensions = []\n",
    "    with open(filename, 'r') as f:\n",
    "        text = f.read()\n",
    "    soup = bs(text,\"html.parser\") # create a beautiful soup object of the given code\n",
    "    table_list = soup.find_all(href=True)\n",
    "    for i in table_list:\n",
    "        if len(i.text) > 3: # ignore the texts that are blank - all the ones\n",
    "                            # we need are .txt files, so at least 3 chars long\n",
    "            extensions.append(i.text)\n",
    "    return extensions    # return the tag name list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then broke down the recipes into their respecive categories: Mexican, Breakfast foods, Fish recipes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories():\n",
    "    ''' Gets all the category variables and saves them in a pickle.'''\n",
    "  \n",
    "    if os.path.exists('categories.pickle'): # checks if the folder already exists\n",
    "        print(\"folder already here: returning contents\")\n",
    "        with open('categories.pickle','rb') as f:\n",
    "            categories = pickle.load(f) # load the saved contents \n",
    "            return categories\n",
    "    # otherwise, scrapes the website, pickles the information, and \n",
    "    # returns the contents\n",
    "    else:\n",
    "        print(\"folder not here yet: creating contents\")\n",
    "        type_list = get_HTML_extensions() # contains the extensions for all\n",
    "                                          # recipes of a specific food type \n",
    "        categories = [t[:-4] for t in type_list] # remove the '.txt' from the list names\n",
    "        # put a space between the words\n",
    "        categories = [re.sub(r\"(?<=\\w)([A-Z])\", r\" \\1\", c) for c in categories]\n",
    "        # then put a space between 'and' and the proceeding word (if there is an 'and')\n",
    "        categories = [re.sub(r\"(?<=)(and )\", r\" \\1\", c) for c in categories]\n",
    "     \n",
    "        with open('categories.pickle','wb') as f:\n",
    "            pickle.dump(categories,f) # save the contents\n",
    "            \n",
    "        return categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{2.B Data Collection: Recipie and Ingredient Cleaning }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our recipes, we turn to the task of making the food they contain worth their weight in data.  Literally.  The more of an ingredient, the bigger the impact it will have on the recipe's final numbers.  Here's how we started to get the goods.\n",
    "\n",
    "We used regex to single out each recipe name, the ingredients  in it, and the serving size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_category_to_recipes(category):\n",
    "    '''Takes in a string of text from a single category of recipes,\n",
    "       and returns a list of strings containing the recipes contained\n",
    "       in that category'''\n",
    "    # create the regex that all the recipes follow- not to give a 'clean'\n",
    "    # cut-off, but rather to separate one recipe from the next.\n",
    "    one_recipe_pattern = re.compile(r\"\\* Exported from MasterCook \\*(.+?)Nutr\\. Assoc\\. : (\\d+?)\", re.DOTALL)\n",
    "    batch = one_recipe_pattern.findall(r) # splits up the text to it's portions,\n",
    "                               # but the formatting is as a list of strings\n",
    "                               # in parenthesies\n",
    "    singles = []\n",
    "    for i in range(len(batch)):\n",
    "        singles.append(batch[i][0]) # unpacks the information to make it accessable\n",
    "    return singles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recipe_info(recipe):\n",
    "    '''Takes in a recipe string, and uses regex to parse out:\n",
    "       the Title, ingredients (as a group), and the serving size\n",
    "       \n",
    "    '''\n",
    "    title_pattern = re.compile(r\"([A-Za-z]{1}[^\\r\\n\\t\\f\\v]*)\") # take the first match\n",
    "    ingredients_batch_pattern = re.compile(r\"--------------------------------(.+?)[\\n\\r]{4}\", re.S)\n",
    "    serving_size_pattern = re.compile(r\"Serving Size  :\\s*(\\d*)\")\n",
    "    \n",
    "    title = title_pattern.search(recipe).group(0) # we only need the first match\n",
    "    serving_size = serving_size_pattern.findall(recipe)[0]\n",
    "    ingredients_batch = ingredients_batch_pattern.findall(recipe)[0]\n",
    "        \n",
    "    return title, serving_size, ingredients_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now to create the dictionary of all this information\n",
    "# to turn it into a pandas dataframe.\n",
    "# Note that for now, I leave the ingredients as a list.\n",
    "\n",
    "# to create the dictionary, it needs to be of the form\n",
    "# {'col1':list(),'col2':list(),...}\n",
    "category_list = []\n",
    "title_list = []\n",
    "serving_size_list = []\n",
    "ingredients_batch_list = []\n",
    "for category,category_name in zip(split_rs,categories):\n",
    "    for recipe in category:\n",
    "        title,serving_size,ingredients_batch = get_recipe_info(recipe)\n",
    "        \n",
    "        # append the appropriate elements to create the needed dictionary\n",
    "        category_list.append(category_name)\n",
    "        title_list.append(title)\n",
    "        serving_size_list.append(serving_size)\n",
    "        ingredients_batch_list.append(ingredients_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'category':category_list,\n",
    "                   'title':title_list,\n",
    "                   'serving size':serving_size_list,\n",
    "                   'ingredients batch':ingredients_batch_list})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ingredients(batch):\n",
    "    l = batch.splitlines()[1:]\n",
    "    measure_pattern = re.compile(r\"(\\d+ \\d/\\d|\\d+/\\d+|\\d+)?(?=  ) +?([A-Aa-z]+)?(?=  )\", re.S)\n",
    "    #TODO: Add the ability to check out the meausrements\n",
    "    ingredient_info_pattern = re.compile(r\".{24}(.*)\")\n",
    "    \n",
    "    ans = []\n",
    "    for i in range(len(l)):\n",
    "        mini = measure_pattern.findall(l[i])\n",
    "        numb = [measure[0] for measure in mini if measure[0] != ''];\n",
    "        if numb: numb = numb[0]; \n",
    "        else: numb = None\n",
    "            \n",
    "        unit = [measure[1] for measure in mini if measure[1] != '']; \n",
    "        if unit: unit = unit[0];\n",
    "        else: unit = None \n",
    "            \n",
    "        # get the string containing info about the ingredient\n",
    "        ing_string = ingredient_info_pattern.findall(l[i])\n",
    "        if ing_string: ing_string = ing_string[0];\n",
    "        else: ing_string = None    \n",
    "        ans.append((numb, unit, ing_string))\n",
    "    return ans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = df['ingredients batch']\n",
    "all_ings = []\n",
    "for i in range(len(b)):\n",
    "    all_ings += [group[2] for group in get_ingredients(b[i])]\n",
    "ingredient_counter = Counter(all_ings)\n",
    "sorted_ings = sorted(ingredient_counter.items(), key=lambda kv: kv[1],reverse=True)\n",
    "\n",
    "\n",
    "df_ings = pd.DataFrame(sorted_ings)\n",
    "# for i in df_ings[:100]:\n",
    "#     print(i)\n",
    "goods = []\n",
    "i = 0\n",
    "while len(goods)<100:\n",
    "#     print(df_ings[0][int(i)])\n",
    "    if '--' not in df_ings[0][int(i)] and 'OR' not in df_ings[0][int(i)]:\n",
    "        goods.append(df_ings[0][int(i)])\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{2.C Data Collection: Ingredient Nutritional Value Scraping}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function takes in the ingredients and looks them up on nutritionvalue.org. and returns a dictionary with the key as the ingredient and the links of the top 3 results as the values.\n",
    "If it does not find any results it will place a None on the value of the key.  That's how we roll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nutrition_website(ingredients):\n",
    "    \"\"\"Use Selenium to enter the given search query into the search bar of\n",
    "    nutrion website and gets links to scrape data\n",
    "\n",
    "    Returns:\n",
    "        (dictonary): urls .\n",
    "    \"\"\"\n",
    "    #initialize variables and chrome\n",
    "    ingredients_dictionary = {}\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(\"https://www.nutritionvalue.org/\")\n",
    "    num_links = 3\n",
    "    try:\n",
    "        for i in ingredients:\n",
    "            try:\n",
    "                \n",
    "                search_bar = browser.find_element_by_name('food_query')\n",
    "                search_bar.clear()\n",
    "                search_bar.send_keys(Keys.CONTROL + \"a\")\n",
    "                search_bar.send_keys(Keys.DELETE)\n",
    "                search_bar.send_keys(i)\n",
    "\n",
    "                search_bar.send_keys(Keys.RETURN)\n",
    "                \n",
    "                words = i.split()\n",
    "                x = \"\"\n",
    "                for n,w in enumerate(words):\n",
    "                    if n == 0:\n",
    "                        x += \".*(?<!food_query=)\"+w+\".*|\"\n",
    "                    else:\n",
    "                        x += \".*(?<!\\+)\"+w+\".*|\"\n",
    "                x = x[:-1]\n",
    "                find = re.compile(x,re.IGNORECASE)\n",
    "            \n",
    "                \n",
    "                # wait for page to load\n",
    "                time.sleep(2)\n",
    "                currentURL = browser.current_url\n",
    "                if \"food_query\" in currentURL:\n",
    "                    \n",
    "                    links = browser.find_elements_by_tag_name('a')\n",
    "                    links = [link.get_attribute(\"href\") for link in links if isinstance(link.get_attribute(\"href\"),str)]\n",
    "                    urls = [link for link in links if len(find.findall(link)) > 0]\n",
    "                    if len(urls) >num_links:\n",
    "                        ingredients_dictionary[i] = urls[:num_links]\n",
    "                    elif len(urls) == 0:\n",
    "                        ingredients_dictionary[i] = None\n",
    "                    else:\n",
    "                        ingredients_dictionary[i] = urls\n",
    "                else:\n",
    "                    ingredients_dictionary[i] = [currentURL]\n",
    "                \n",
    "\n",
    "            except NoSuchElementException:\n",
    "                print(\"could not find the search bar!\")\n",
    "                print(i)\n",
    "                return ingredients_dictionary\n",
    "    # close window\n",
    "    finally:\n",
    "        browser.close()\n",
    "    # list with all the links\n",
    "    return ingredients_dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the dictionary of ingredients and sources we ran the following code that collects the nutritional value of all ingredients and builds a dictionary of dictionaries. the main key is the ingredient and the value is a dictionary containing the nutriotional value (ex: Calories: 300, Protein: 10g, etc). \n",
    "\n",
    "In the given chance that there is an error in the process, the code will print out the ingredient, the link, and the error. If the error is IDK it can be checked manually and it can be improved on (This is how we have improved our code). The ingredients that have errors are then added to \"error_items\" which can be used for debuging purposes as well.\n",
    "\n",
    "The code also returns a set of all the links that have been used in order to not over use the server of the website (it can be passed as the second argument)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nutrition_value(dictionary,set_of_links = set()):\n",
    "    \"\"\"Takes in a dictionary with ingredients as keys\n",
    "    look through the websites and scrape the nutritional value\"\"\"\n",
    "    error_items =[]\n",
    "    df_d = dict()\n",
    "    browser = webdriver.Chrome()\n",
    "    try:\n",
    "        for k,v in zip(dictionary.keys(),dictionary.values()):\n",
    "            if v is None:\n",
    "                df_d[k] = {}\n",
    "            else:    \n",
    "                for l in v:\n",
    "                    try:\n",
    "                        if l is None:\n",
    "                            print(f\"No Website for: {k}\")\n",
    "                            print()\n",
    "                        elif l in set_of_links:\n",
    "                            print(f\"Duplicates for {k}\")\n",
    "                        else:\n",
    "                            browser.get(l)\n",
    "                            time.sleep(5)\n",
    "                            # name of ingredient\n",
    "                            name = browser.find_elements_by_tag_name('h1')[0]\n",
    "                            name = name.text\n",
    "\n",
    "                            #setting up nutritional values\n",
    "                            nut = dict()\n",
    "                            c = \"tbody\"\n",
    "                            tables = browser.find_elements_by_tag_name(c)\n",
    "                            #### For essentials [4]\n",
    "                            ser_cal = tables[4].text.split('\\n')\n",
    "                            # Serving Size\n",
    "                            nut[ser_cal[1][:12]] = ser_cal[1][13:]\n",
    "                            #Calories\n",
    "                            nut[ser_cal[3][:8]] = ser_cal[3][9:]\n",
    "\n",
    "                            #### For all others [7-13]\n",
    "                            n_v = re.compile('\\s*(.*)\\s([0-9]+\\.[0-9]+\\s\\w+)')\n",
    "                            for i in range(7,14):\n",
    "\n",
    "                                nutrient_value = [n_v.findall(t) for t in tables[i].text.split('\\n') if len(t) >0]\n",
    "                                for t in nutrient_value:\n",
    "                                    if len(t)>0:\n",
    "                                        nut[t[0][0]] = t[0][1]\n",
    "\n",
    "                            df_d[name] = nut\n",
    "                            set_of_links.add(l)\n",
    "                        \n",
    "                    except IndexError as e:\n",
    "                        error_items.append(k)\n",
    "                        print(k)\n",
    "                        print(l)\n",
    "                        print(e)\n",
    "                        print()\n",
    "                    except:\n",
    "                        error_items.append(k)\n",
    "                        print(k)\n",
    "                        print(\"IDK\")\n",
    "                        print(l)\n",
    "                        print()\n",
    "            \n",
    "    finally:\n",
    "        browser.close()\n",
    "    # list with all the links\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame.from_dict(df_d,'index')\n",
    "    return df, error_items, set_of_links\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{2.D Data Collection: Ingredient Nutritional Value Cleaning}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic cleaning includes taking care of any columns that are singletons for a data point, changing texts to floats, and documenting units and converting them to similar units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# floats for calories\n",
    "df = ds\n",
    "df[\"Calories\"] = df['Calories'].astype('float')/100\n",
    "df = df.drop(columns = [\"18\",\"Adjusted Protein\"])\n",
    "# changing the entries to floats and keeping track of units\n",
    "clean_columns = list(df.columns)\n",
    "other_units = []\n",
    "g_units = []\n",
    "\n",
    "\n",
    "for j,c in enumerate(clean_columns):\n",
    "    if c ==  \"Calories\":\n",
    "        \n",
    "        pass\n",
    "    else:    \n",
    "        # float values\n",
    "        n = re.compile(r\"(^\\d*\\.?\\d*)\\s(\\w+)\")\n",
    "        \n",
    "        ### changing Na for 0's, to go back change 0 to np.nan\n",
    "        num = np.array([float(n.findall(i)[0][0]) if isinstance(i,str) else 0 for i in df[c].values])\n",
    "        mes = np.array([n.findall(i)[0][1] if isinstance(i,str) else 'g' for i in df[c].values])\n",
    "\n",
    "        # measurements to grams\n",
    "        mask_mg = (mes == 'mg')/10\n",
    "        mask_mcg = (mes == \"mcg\")/10000\n",
    "        mask_g = (mes == \"g\")*.01\n",
    "        mask = mask_mg + mask_mcg +mask_g\n",
    "        \n",
    "\n",
    "        mask += (mask==0)*-1\n",
    "\n",
    "        if sum(mask < 0) >1 :\n",
    "            df[c] = num/100\n",
    "            other_units.append(c)\n",
    "\n",
    "        else:\n",
    "            df[c] = num*mask\n",
    "            g_units.append(c)\n",
    "\n",
    "# append new features\n",
    "df.rename(columns = {c:c+ \" (g)\" for c in g_units if c!= \"Calories\"},inplace = True)\n",
    "df.rename(columns = {c:c+ \" (IU)\" for c in other_units}, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{3. Data Visualization}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our final, epic, humungo sized dataframe!  What should we ask it?\n",
    "\n",
    "\"Oh Oracle, tell me: from the many ingredients that we have, is there a correlation among the nutritional contents they contain?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"corr_m.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oracle says not realy...\n",
    "\n",
    "Ok.  How about this:\n",
    "\"Oh Oracle, tell me: I fear getting fat over christmas.  From what should I abstain?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fat.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oracle clearly has prohibited popcorn and easter food... That's ok.  It's Christmas time, and I like my gingerbread houses.\n",
    "\n",
    "I also want to get swole.  Gains matter.  What should I beef up on? (Pun very much intended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Protein_comp.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like I should stick to seafood... doesn't seem very seasonal though.  (Unless I lived in sea-attle!)\n",
    "\n",
    "Ok, well, being that I'm only going to eat Beef, Pork, or Poultry for my protein source, and that some cousin of mine might be vegatarian now, I'll just compare those options.  They look relatively similar overall... I wonder if they have a different makeup?  Wiki-legend and sources confirm that protein is made of amino acids... lets see how they compare!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"amino_acids.png\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's funny... they look about the same!  I wonder how those values compare to the average of all the recipes we've looked at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"AA_ovarall.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you have it.  The same pattern yet again.  Well, if in proportion, my amino acid count isn't going to differ much, I'll probably stick with crawfish.  Like I said, 'Gains'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{4. Conclusion}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many new discoveries. Have crawfish for your gains, and lose some weight over your pork or lamb holiday meal.\n",
    "\n",
    "Eat, drink, and be merry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merry Christmas :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
