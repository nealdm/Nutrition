{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation and Overview of Data:\n",
    "\n",
    "## Project Purpose:\n",
    "\n",
    "We want to compare recipies of different types by looking for trends in their nutritional value.\n",
    "\n",
    "\n",
    "## Objective Questions:\n",
    "\n",
    "What kind of protein based meal gives me the most protein per serving?\n",
    "\n",
    "What kind of recipe gives the largest 'healthy fat' to 'unhealthy fat' ratio? (And visa versa)\n",
    "\n",
    "Is there a correlation between the protein, mineral, and vitamin contents per serving of a recipe?\n",
    "\n",
    "\n",
    "## Background knowledge and resources: \n",
    "\n",
    "#### What has been done\n",
    "\n",
    "There are already lots of resources easily available online to help you search for recipes.  Many of these allow you to search recipes based on the type of recipe you want (comfort food, quick and easy, Mexican etc.), and some let you search for recipes based on what ingredients you have in your fridge, allowing the user to say \"I have milk, eggs, rasins, chicken, and rice- What can I make?\"  However, no recipe search databases are readily available that give nutritional value of the recipes.\n",
    "\n",
    "#### How this is different\n",
    "\n",
    "In this project, we are interested in creating a table that will help us sort through recipes based on nutritional value so that in a future project we can work on finding a set of recipes to match specific nutritional needs.  The table will enable us to answer the above questions. \n",
    "\n",
    "#### Resources\n",
    "\n",
    "We will create the table with the information about the nutritional values of recipes by starting with two specific resources.  \n",
    "\n",
    "The first resource pulls from recipe text files that used to be offered at the Recipe Library at MasterCook.com.  It is a website that contains a list of links, where each link leads to a text file containing many recipes that all fit in the same 'category' of food.  This is the resource we will be scraping from in order to collect a resonable sample of recipes.  \n",
    "\n",
    "The second resource is a website called nutritionvalue.org (NutritionValue) that we will use to search for the nutritional content of the ingredients used in the recipes mentioned above.  NutritionValue allows us to search, for example, \"banana\", which will then return a list of links of all the various brands of bananas that it has in the database.  Each one of these links then leads to a page that contains all the nutritional information (calorie content, protein content, vitamins, minerals etc.) of that particular banana selection.\n",
    "\n",
    "$$ Website\\ Containing\\ Recipe\\ Collection\\ Text\\ Files $$ | $$ Website\\ to\\ Collect\\ Individual\\ Ingredient\\ Nutrition $$ \n",
    "- | -\n",
    "![title](recipe_website.png) | ![title](nutrition_website.png)\n",
    "\n",
    "\n",
    "We will use these two resources to collect the needed information to create a table that will help us answer our objective questions.\n",
    "\n",
    "#### Validity of Resources\n",
    "\n",
    "It is important to note that both of these resources are appropriate for the question at hand.  \n",
    "\n",
    "The first is valid because it contains recipes that were created by various users, and offers a variety of recipes.  Therefore, the sample of recipies is well distributed.  It is also a suitable source because it offers a particularly easy way to download the a large number of recipes with consistant formatting.\n",
    "\n",
    "The second resource is valid because it pulls its data from the USDA National Nutrient Database for Standard Reference.  It has a broad range of ingreadients available to search, and is also very detailed in it's nutritional content breakdown, which makes it suitable for this project.\n",
    "\n",
    "\n",
    "## Table Construction Overview:\n",
    "\n",
    "\n",
    "In order to answer the objective questions, we need to create a table containing the relevent information. We will construct this table after the following format:\n",
    "\n",
    "|Category|Recipe|Serving Size|Salt|Sugar|...|Ingredient n|\n",
    "|------|------|------|------|------|------|------|\n",
    "|Chowders|Clam Chowder|4|...|...|...|...|\n",
    "\n",
    "\n",
    "Each recipe will have an associated category, serving size, and ingredient columns.  Each ingredient column shown here actually represents a 'batch' of many columns that contain: one column for how much of the ingredient is present, and a column for how much of each nutritional property (protein content, vitamin content, Calorie count etc) is contained per 100 grams of that ingredient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection and Cleaning:\n",
    "\n",
    "In order to form the desired table, we first need to collect the recipes and their associated ingredient nutritional information.  The process for this is broken down into two steps: recipe collection and ingredient information collection.\n",
    "\n",
    "## Recipe Collection Procedure:\n",
    "\n",
    "### Link Collection\n",
    "\n",
    "In order to collect the information for the recipes, we need to first scrape all the links that will lead to the text files of each category.  Each link contains a text file that looks like a list of recipes such as the picture on the left below.  The recipies consist of the title, ingredients, and an explanation portion on how to 'make' the recipe.  The portion of each recipe we will be using is just the quantitative portion, for which an example is given at the right.\n",
    "\n",
    "$$ Portion\\ of\\ one\\ Text\\ File $$ | $$ Example\\ Recipe $$ \n",
    "- | -\n",
    "<img src=\"recipesamples.png\" width=\"800\" height=\"400\">| <img src=\"recipeexample.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "We need to first thus scrape the website of the appropriate links, and then scrape each link of the list of recipes it contains.  Here is some code that does that:\n",
    "\n",
    "![title](recipe_scrape_code.png)\n",
    "\n",
    "### Create Recipe Categories\n",
    "\n",
    "We then use the titles of each scraped link to make a list of the category names.\n",
    "\n",
    "![title](find_rcategories_code.png)\n",
    "\n",
    "In all, there are 85 different categories, and they have names such as 'Alcoholic Beverages', 'All Appetizer Recipes', 'All Beverage Recipes', 'All Bread Recipes', 'All Breakfast Recipes', ..., 'Biscuits and Scones', 'Bread Machine Recipes', 'Brownies',... and many more.  \n",
    "\n",
    "### Separate Individual Recipes\n",
    "\n",
    "We then separate each category's single long string of recipes into a list of strings each containing one recipe.  We use Regex to do this in the following manner:\n",
    "\n",
    "![title](separate_recipes_code.png)\n",
    "\n",
    "### Get Ingredient Info\n",
    "\n",
    "Now we take each recipe, and use Regex to separate out the title, serving size, and 'ingredients batch' (which is a string contianing the ingredients, their quantities and units of measurement)\n",
    "\n",
    "![title](get_recipe_info_code.png)\n",
    "\n",
    "### Create first basic table\n",
    "\n",
    "Now we want to create a table that uses the data that we are able to successfully scrape.  \n",
    "\n",
    "![title](create_recipe_df_code.png)\n",
    "\n",
    "Here's the table that we end up with here:\n",
    "\n",
    "![title](basic_table_head.png)\n",
    "\n",
    "### Dropping Missformatted Recipes\n",
    "\n",
    "Note that we drop some of the recipes because they don't fit the correct formatting, and there are few enough of them that they don't pose too much of a loss to the overall effort of creating a table that represents the nutritional values of various recipes.  It was interesting to note however that the category with the most 'erroneous' recipes was that of 'Sourdough bread' - there were multiple instances where people would give 'hints' about how to care for the bread, rather than actually providing a recipe!\n",
    "\n",
    "#### $$ Example\\ of\\ poorly\\ formatted\\ recipe $$ \n",
    "\n",
    "![title](bad_recipe.png)\n",
    "\n",
    "### Parsing the Ingredient Information\n",
    "\n",
    "We now need to go through the recipe data and separate it out into it's constituant parts: separate the ingredient name from the quantity from the unit.  Here is a sample of code used to do that:\n",
    "\n",
    "![title](separate_ingredients_code.png)\n",
    "\n",
    "### Normalize ingreadient names\n",
    "\n",
    "Next we need to have a way of normalizing the ingredient names.  This is because many ingredients are presented slightly differently.  For example, brown sugar, dark brown sugar, lightly-packed brown sugar, and Brn. sugar could all be reasonably categorized into the same ingredient class: \"brown sugar.\"\n",
    "\n",
    "In order to do this, we first normalized all the ingredients to make them \n",
    "lowercase.  \n",
    "\n",
    "We then created 'class' categories by looking at the most common 200 ingredient types and creating a category for each of them.  We selected the top 200 categories because the 200th most common ingredient is only used in about 12 of the roughly 5,500 recipes we scraped from, and there were over 10,000 ingredients total.  Thus we decided the tail of ingredients that were used very little could be cut off, and the majority of the informational content would still be observed in the table we are creating. \n",
    "\n",
    "We ultimately created a dictionary that mapped the raw ingredient name to the class to which it pertains. (See pertinant Code in the Appendix)\n",
    "\n",
    "For the classes that did not fit into a category, we put their information in an 'OTHER' class, allowing us to cleanly sort through the data.\n",
    "\n",
    "### Create Second Basic Table\n",
    "\n",
    "With the categories created, we can now create the basic layout of our recipe-ingredients table.  The following is the head of this dataframe, with the ingredients separated out, but the nutritional information of each ingredient not yet present.\n",
    "\n",
    "![title](sorted_dataframe_head.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingredient Information Collection Procedure:\n",
    "\n",
    "Now that we have the individual ingredients we can start collecting nutritional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaper we build for this task searches each ingredient on nutritionvalue.org and takes the first available 3 links. \n",
    "Ex: Oats\n",
    "![title](nutrition_value_query.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE: Scraper 1\n",
    "def nutrition_website(ingredients):\n",
    "    \"\"\"Use Selenium to enter the given search query into the search bar of\n",
    "    nutrion website and gets links to scrape data\n",
    "\n",
    "    Returns:\n",
    "        (dictonary): urls .\n",
    "    \"\"\"\n",
    "    #initialize variables and chrome\n",
    "    ingredients_dictionary = {}\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(\"https://www.nutritionvalue.org/\")\n",
    "    num_links = 3\n",
    "    try:\n",
    "        for i in ingredients:\n",
    "            try:\n",
    "                #navigate\n",
    "                search_bar = browser.find_element_by_name('food_query')\n",
    "                search_bar.clear()\n",
    "                search_bar.send_keys(Keys.CONTROL + \"a\")\n",
    "                search_bar.send_keys(Keys.DELETE)\n",
    "                search_bar.send_keys(i)\n",
    "\n",
    "                search_bar.send_keys(Keys.RETURN)\n",
    "                \n",
    "                words = i.split()\n",
    "                x = \"\"\n",
    "                for n,w in enumerate(words):\n",
    "                    if n == 0:\n",
    "                        x += \".*(?<!food_query=)\"+w+\".*|\"\n",
    "                    else:\n",
    "                        x += \".*(?<!\\+)\"+w+\".*|\"\n",
    "                x = x[:-1]\n",
    "                find = re.compile(x,re.IGNORECASE)\n",
    "            \n",
    "                \n",
    "                # wait for page to load\n",
    "                time.sleep(2)\n",
    "                currentURL = browser.current_url\n",
    "                if \"food_query\" in currentURL:\n",
    "                    \n",
    "                    links = browser.find_elements_by_tag_name('a')\n",
    "                    links = [link.get_attribute(\"href\") for link in links if isinstance(link.get_attribute(\"href\"),str)]\n",
    "                    urls = [link for link in links if len(find.findall(link)) > 0]\n",
    "                    if len(urls) >num_links:\n",
    "                        ingredients_dictionary[i] = urls[:num_links]\n",
    "                    elif len(urls) == 0:\n",
    "                        ingredients_dictionary[i] = None\n",
    "                    else:\n",
    "                        ingredients_dictionary[i] = urls\n",
    "                else:\n",
    "                    ingredients_dictionary[i] = [currentURL]\n",
    "                \n",
    "\n",
    "            except NoSuchElementException:\n",
    "                print(\"could not find the search bar!\")\n",
    "                print(i)\n",
    "                return ingredients_dictionary\n",
    "    # close window\n",
    "    finally:\n",
    "        browser.close()\n",
    "    # list with all the links\n",
    "    return ingredients_dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These links are collected in a dictionary with the ingredient as its respective value.\n",
    "\n",
    "Then we are ready to get the nutritional value!\n",
    "\n",
    "Continuing with our oats example, these are a few of the tables from which we got our nutrition data\n",
    "\n",
    "![title](nutrition_values_oats.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for scraper\n",
    "def nutrition_value(dictionary,set_of_links = set()):\n",
    "    \"\"\"Takes in a dictionary with ingredients as keys\n",
    "    look through the websites and scrape the nutritional value\"\"\"\n",
    "    error_items =[]\n",
    "    df_d = dict()\n",
    "    browser = webdriver.Chrome()\n",
    "    try:\n",
    "        for k,v in zip(dictionary.keys(),dictionary.values()):\n",
    "            if v is None:\n",
    "                df_d[k] = {}\n",
    "            else:    \n",
    "                for l in v:\n",
    "                    try:\n",
    "                        if l is None:\n",
    "                            print(f\"No Website for: {k}\")\n",
    "                        elif l in set_of_links:\n",
    "                            print(f\"Duplicates for {k}\")\n",
    "                        else:\n",
    "                            browser.get(l)\n",
    "                            time.sleep(5)\n",
    "                            # name of ingredient\n",
    "                            name = browser.find_elements_by_tag_name('h1')[0]\n",
    "                            name = name.text\n",
    "\n",
    "                            #setting up nutritional values\n",
    "                            nut = dict()\n",
    "                            c = \"tbody\"\n",
    "                            tables = browser.find_elements_by_tag_name(c)\n",
    "                            #### For essentials [4]\n",
    "                            ser_cal = tables[4].text.split('\\n')\n",
    "                            # Serving Size\n",
    "                            nut[ser_cal[1][:12]] = ser_cal[1][13:]\n",
    "                            #Calories\n",
    "                            nut[ser_cal[3][:8]] = ser_cal[3][9:]\n",
    "\n",
    "                            #### For all others [7-13]\n",
    "                            n_v = re.compile('\\s*(.*)\\s([0-9]+\\.[0-9]+\\s\\w+)')\n",
    "                            for i in range(7,14):\n",
    "                                nutrient_value = [n_v.findall(t) for t in tables[i].text.split('\\n') if len(t) >0]\n",
    "                                for t in nutrient_value:\n",
    "                                    if len(t)>0:\n",
    "                                        nut[t[0][0]] = t[0][1]\n",
    "\n",
    "                            df_d[name] = nut\n",
    "                            set_of_links.add(l)\n",
    "                        \n",
    "                    except IndexError as e:\n",
    "                        error_items.append(k)\n",
    "                        print(f\"ingredient:{k}, error: {e}, link: {l}\")\n",
    "                    except:\n",
    "                        error_items.append(k)\n",
    "                        print(f\"ingredient:{k}, error: IDK, link: {l}\")\n",
    "            \n",
    "    finally:\n",
    "        browser.close()\n",
    "    # list with all the links\n",
    "    df = pd.DataFrame.from_dict(df_d,'index')\n",
    "    return df, error_items, set_of_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scraper would print out any errors it ran into and the link that caused it. Each link was inspected to make sure we weren't losing any important information. All printed out links were to other parts of the website that were gathered depending on the words we were searching. \n",
    "\n",
    "In order to not abuse the websites information we have also added a argument that skips the search if it has already been pulled.\n",
    "\n",
    "Ones we collected the data we analysed it for errors. There were two columns that had only one value out of all the ingredients - the number \"18\" and \"adjusted Protein\". For this reason we dropped those columns. \n",
    "\n",
    "The rest of the data was cleaned by making all values floats, converting values to grams (g) and making serving sizes be 1g for all ingredients.\n",
    "\n",
    "Vitamin A and D were a special case because they were measured in International Units (IU) so we converted them to grams\n",
    "\n",
    "Lastly we engineered a columns for all minerals and vitamins for future investigation of trends in nutrition - Whether you can get all the nutrition you need from certain foods or recipes.\n",
    "\n",
    "Code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scraper would print out any errors it ran into and the link that caused it. Each link was inspected to make sure we weren't losing any important information. All printed out links were to other parts of the website that were gathered depending on the words we were searching. \n",
    "\n",
    "In order to not abuse the websites information we have also added a argument that skips the search if it has already been pulled.\n",
    "\n",
    "Ones we collected the data we analysed it for errors. There were two columns that had only one value out of all the ingredients - the number \"18\" and \"adjusted Protein\". For this reason we dropped those columns. \n",
    "\n",
    "The rest of the data was cleaned by making all values floats, converting values to grams (g) and making serving sizes be 1g for all ingredients.\n",
    "\n",
    "Vitamin A and D were a special case because they were measured in International Units (IU) so we converted them to grams\n",
    "\n",
    "Lastly we engineered a columns for all minerals and vitamins for future investigation of trends in nutrition - Whether you can get all the nutrition you need from certain foods or recipes.\n",
    "\n",
    "Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# floats for calories\n",
    "df = ds\n",
    "df[\"Calories\"] = df['Calories'].astype('float')/100\n",
    "df = df.drop(columns = [\"18\",\"Adjusted Protein\"]) # contains singleton\n",
    "\n",
    "for j,c in enumerate(list(df.columns)):\n",
    "    if c ==  \"Calories\":     \n",
    "        pass\n",
    "    else:    \n",
    "        n = re.compile(r\"(^\\d*\\.?\\d*)\\s(\\w+)\")     # float values\n",
    "        \n",
    "        ### changing Na for 0's, to go back change 0 to np.nan\n",
    "        num = np.array([float(n.findall(i)[0][0]) if isinstance(i,str) else 0 for i in df[c].values])\n",
    "        mes = np.array([n.findall(i)[0][1] if isinstance(i,str) else 'g' for i in df[c].values])\n",
    "\n",
    "        # messurements\n",
    "        mask_mg = (mes == 'mg')/10\n",
    "        mask_mcg = (mes == \"mcg\")/10000\n",
    "        mask_g = (mes == \"g\")*.01\n",
    "        mask = mask_mg + mask_mcg +mask_g\n",
    "        mask += (mask==0)*-1\n",
    "\n",
    "        if sum(mask < 0) >1 :\n",
    "            df[c] = num/100\n",
    "        else:\n",
    "            df[c] = num*mask\n",
    "\n",
    "df[\"Vitamin A\"] *= 0.6/1000000\n",
    "df[\"Vitamin D\"] *= 0.025/1000000\n",
    "\n",
    "vitamins=['Choline','Niacin','Pantothenic acid','Riboflavin','Thiamin',\n",
    "          'Vitamin A','Vitamin B12','Vitamin B6','Vitamin C','Vitamin D','Vitamin E','Vitamin K']\n",
    "minerals = ['Calcium, Ca','Copper, Cu','Iron, Fe',    'Magnesium, Mg',\n",
    "            'Manganese, Mn','Phosphorus, P','Potassium, K','Selenium, Se','Sodium, Na','Zinc, Zn']\n",
    "# append new features\n",
    "df['Vitamins'] = df[vitamins].sum(axis=1)\n",
    "df['Minerals'] = df[minerals].sum(axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that both data sets are clean and contain featured columns we merged them to form a giant sparse dataframe of the form *(see appendix)\n",
    "\n",
    "One of our biggest challenges was to sort through the different quantities and converting them to grams. We have come up with a temporal solution why converts abstract measurements into the equivalent in grams for the most used ingredient. \n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = {c:[] for c in set(u.values())}\n",
    "# g/ units\n",
    "conv['gram'] = 1\n",
    "conv['liter'] = 1000\n",
    "conv[\"gallon\"] = 0.00026417205\n",
    "conv['lbs'] = 453.592\n",
    "conv['milliliter'] = 1\n",
    "conv['oz']= 28.34\n",
    "conv['kilogram'] = 1000\n",
    "conv['tablespoon'] = 14.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are continually searching for a solution to this problem. Nevertheless, We proceed with the analysis so that when an improvement to this method is found we can get better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Quality and Robustness:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Visualization and Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](hefty_head.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for creating Ingredient Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next singularize all the words in all_ingredients using the inflect package\n",
    "ingredient_class_dict1 = []\n",
    "stripped_ingredients = []\n",
    "p = inflect.engine() # this will be used to singularize plural words\n",
    "for ingredient_pair in ingredient_class_dict:\n",
    "    # for each string, strip the non alphanumeric chars and\n",
    "    # singularize the word if it is plural, then rejoin the string\n",
    "    alph = re.compile('[\\W_]+')\n",
    "#     word = ingredient_pair[1]\n",
    "    word_list = [p.singular_noun(alph.sub('', word))\n",
    "                 if p.singular_noun(alph.sub('', word)) \n",
    "                 else \n",
    "                 (alph.sub('', word) if word not in not_foods\n",
    "                  else None)                 \n",
    "                 for word in ingredient_pair[1].split()]\n",
    "#     if not p.singular_noun(alph.sub('', word)) and word in not_foods: # not necessary: just seeing what I'm throwing out.\n",
    "#         print(ingredient_pair[0])\n",
    "    no_nones = list(filter(None, word_list))\n",
    "    word = \" \".join(map(str,no_nones))\n",
    "    # set the revised word as the second element in the pair\n",
    "    ingredient_class_dict1.append((ingredient_pair[0],word))\n",
    "    # also I'll save the word here so that I can sort through all words easily\n",
    "    if word not in not_foods:\n",
    "        stripped_ingredients.append(word)\n",
    "stripped_ingredients[:10]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ingredient_classes(sorted_ings):\n",
    "    ''' Takes in a list of sorted ingredients,\n",
    "     and gets all the ingredient 'head' variables and saves them in a pickle.\n",
    "     returns a list of sublists, where the first element of each sublist is the 'head' \n",
    "     variable, and each other element are the variables that fall under it's 'ingredient type'.\n",
    "     '''\n",
    "    if os.path.exists('ingredient_name_groups.pickle'): # checks if the folder already exists\n",
    "        print(\"pickle already here: returning contents\")\n",
    "        with open('ingredient_name_groups.pickle','rb') as f:\n",
    "            name_groups = pickle.load(f) # load the saved contents \n",
    "            return name_groups\n",
    "    # otherwise, scrapes the website, pickles the information, and \n",
    "    # returns the contents\n",
    "    else:\n",
    "        print(\"pickle not here yet: creating contents\")\n",
    "        name_groups = []\n",
    "        check_list = set()\n",
    "        j = 0\n",
    "        while len(name_groups) < 200:\n",
    "            current_item = sorted_ings[j]\n",
    "            if current_item not in check_list:\n",
    "                check_list.add(current_item)\n",
    "                # use fuzzywuzzy to appropriately add elements to the same list as the 'head' element\n",
    "                name_groups.append([sorted_ings[i][0] for i in range(j,2000) if fuzz.partial_ratio(current_item,sorted_ings[i]) > 76])\n",
    "            j += 1\n",
    "\n",
    "        with open('ingredient_name_groups.pickle','wb') as f:\n",
    "            pickle.dump(name_groups,f) # save the contents\n",
    "\n",
    "        return name_groups  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for hefty DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets mask for ingredients (CHECK MASK BEING USED)\n",
    "col = list(neal_df.columns) \n",
    "mask_class = np.array([i for i in col if i[:6] == \"class_\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hefty_columns = [nut_element+\"_\"+category[6:] for category in mask_class for nut_element in df.columns]\n",
    "hefty_columns += [\"Total \"+ nut_element for nut_element in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hefty_df = pd.DataFrame(columns=hefty_columns)\n",
    "num_recipies = len(list(neal_df.index))\n",
    "for num, recipe in enumerate(neal_df.index):\n",
    "    r = neal_df.iloc[recipe]\n",
    "    r_contents = r[mask_class].values != None\n",
    "    ingredients = r[mask_class][r_contents].values\n",
    "    hefty_df.loc[len(hefty_df)] = 0\n",
    "    if num% 200 == 0:\n",
    "        print(f\"{num/num_recipies}%\")\n",
    "    for ing in ingredients:\n",
    "        quantity = st_to_fl(r['quant_' + ing])\n",
    "        unit_factor = conv[r['unit_' + ing]]\n",
    "        conversion = quantity*unit_factor  \n",
    "#         print(conversion)\n",
    "        most_similar_ing = process.extractOne(ing,df.index)[0]\n",
    "#         print(ing)\n",
    "        col = list(hefty_df.columns) \n",
    "        hefty_mask = np.array([i for i in col if i[-(len(ing)+1):] == \"_\"+ing])\n",
    "        for m,i,c in zip(hefty_mask, df.loc[most_similar_ing].values,df.columns):\n",
    "            value = i*conversion\n",
    "            hefty_df.loc[num][m] = value\n",
    "            hefty_df.loc[num][\"Total \" + c] += value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for making similar units the same by human input (for first time seen only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing units by hand\n",
    "# u = dict()\n",
    "# for i,rec in enumerate(neal_df[mask].values):\n",
    "#     for j,mes in enumerate(rec):\n",
    "#         if mes in u:\n",
    "#             pass\n",
    "#         else:\n",
    "#             print(neal_df[mask].columns[j])\n",
    "#             print(mes)\n",
    "#             u[mes] = input()\n",
    "#             print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
